{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project: Train Balancing a pendulum\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import gym\n",
    "import util\n",
    "from agents.agent import DDPG\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "env = gym.make('Pendulum-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State space: Box(3,)\n",
      "- low: [-1. -1. -8.]\n",
      "- high: [1. 1. 8.]\n",
      "State space samples:\n",
      "[[ 0.09762701  0.43037874  1.644214  ]\n",
      " [ 0.08976637 -0.1526904   2.3343058 ]\n",
      " [-0.12482557  0.78354603  7.4186044 ]\n",
      " [-0.23311697  0.5834501   0.46231872]\n",
      " [ 0.13608912  0.85119325 -6.863423  ]]\n"
     ]
    }
   ],
   "source": [
    "# Explore state (observation) space\n",
    "print(\"State space:\", env.observation_space)\n",
    "print(\"- low:\", env.observation_space.low)\n",
    "print(\"- high:\", env.observation_space.high)\n",
    "\n",
    "# Generate some samples from the state space \n",
    "print(\"State space samples:\")\n",
    "print(np.array([env.observation_space.sample() for i in range(5)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action space: Box(1,)\n",
      "Action space samples:\n",
      "[[-1.6514828]\n",
      " [-1.9191264]\n",
      " [ 1.3304794]\n",
      " [ 1.112627 ]\n",
      " [ 1.4800485]]\n"
     ]
    }
   ],
   "source": [
    "# Explore the action space\n",
    "print(\"Action space:\", env.action_space)\n",
    "\n",
    "# Generate some samples from the action space\n",
    "print(\"Action space samples:\")\n",
    "print(np.array([env.action_space.sample() for i in range(5)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Define the Task, Design the Agent, and Train Your Agent!\n",
    "\n",
    "Amend `task.py` to specify a task of your choosing.  If you're unsure what kind of task to specify, you may like to teach your quadcopter to takeoff, hover in place, land softly, or reach a target pose.  \n",
    "\n",
    "After specifying your task, use the sample agent in `agents/policy_search.py` as a template to define your own agent in `agents/agent.py`.  You can borrow whatever you need from the sample agent, including ideas on how you might modularize your code (using helper methods like `act()`, `learn()`, `reset_episode()`, etc.).\n",
    "\n",
    "Note that it is **highly unlikely** that the first agent and task that you specify will learn well.  You will likely have to tweak various hyperparameters and the reward function for your task until you arrive at reasonably good behavior.\n",
    "\n",
    "As you develop your agent, it's important to keep an eye on how it's performing. Use the code above as inspiration to build in a mechanism to log/save the total rewards obtained in each episode to file.  If the episode rewards are gradually increasing, this is an indication that your agent is learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode =  437"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-b9b9fc3587ee>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     37\u001b[0m             \u001b[1;31m#print(next_state, reward, done, _)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m             \u001b[1;31m#print(done)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m             \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m             \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\!Weiterbildung\\!DLND\\!finalProject\\RL-Quadcopter-2\\agents\\agent.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action, reward, next_state, done)\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m             \u001b[0mexperiences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexperiences\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m         \u001b[1;31m# Roll over last state and action\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\!Weiterbildung\\!DLND\\!finalProject\\RL-Quadcopter-2\\agents\\agent.py\u001b[0m in \u001b[0;36mlearn\u001b[1;34m(self, experiences)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m         \u001b[1;31m# Soft-update target models\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 98\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoft_update\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcritic_local\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcritic_target\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     99\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoft_update\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactor_local\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactor_target\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\!Weiterbildung\\!DLND\\!finalProject\\RL-Quadcopter-2\\agents\\agent.py\u001b[0m in \u001b[0;36msoft_update\u001b[1;34m(self, local_model, target_model)\u001b[0m\n\u001b[0;32m    107\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m         \u001b[0mnew_weights\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtau\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mlocal_weights\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtau\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mtarget_weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 109\u001b[1;33m         \u001b[0mtarget_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_weights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\topology.py\u001b[0m in \u001b[0;36mset_weights\u001b[1;34m(self, weights)\u001b[0m\n\u001b[0;32m   2029\u001b[0m                 \u001b[0mtuples\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2030\u001b[0m             \u001b[0mweights\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mweights\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnum_param\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2031\u001b[1;33m         \u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_set_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtuples\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2032\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2033\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36mbatch_set_value\u001b[1;34m(tuples)\u001b[0m\n\u001b[0;32m   2371\u001b[0m             \u001b[0massign_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0massign_op\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2372\u001b[0m             \u001b[0mfeed_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0massign_placeholder\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2373\u001b[1;33m         \u001b[0mget_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0massign_ops\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2374\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2375\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    898\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 900\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    901\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1133\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1135\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1136\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1314\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1316\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1317\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1318\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1320\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1321\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1322\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1323\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1324\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1305\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1307\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1309\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1407\u001b[0m       return tf_session.TF_SessionRun_wrapper(\n\u001b[0;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1409\u001b[1;33m           run_metadata)\n\u001b[0m\u001b[0;32m   1410\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1411\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# This is nice if you interupt the learning while rendering.\n",
    "# This will automatically close the previous session.\n",
    "try:\n",
    "    agent.task.close()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "num_episodes = 1000\n",
    "\n",
    "agent = DDPG(env, gym=True)\n",
    "\n",
    "\n",
    "# general configuration\n",
    "display_graph = True\n",
    "display_freq = 1\n",
    "file_output = 'data.txt'                         # file name for saved results\n",
    "\n",
    "labels = ['time', 'x', 'y', 'z', 'phi', 'theta', 'psi', 'x_velocity',\n",
    "          'y_velocity', 'z_velocity', 'phi_velocity', 'theta_velocity',\n",
    "          'psi_velocity', 'rotor_speed1', 'rotor_speed2', 'rotor_speed3', 'rotor_speed4', 'reward']\n",
    "\n",
    "total_rewards = []\n",
    "\n",
    "for i_episode in range(1, num_episodes+1):\n",
    "    \n",
    "    results = {x : [] for x in labels}\n",
    "    # Run the simulation, and save the results.\n",
    "    with open(file_output, 'w') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow(labels)\n",
    "\n",
    "        state = agent.reset_episode() # start a new episode\n",
    "    \n",
    "        while True:\n",
    "            action = agent.act(state) \n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            #print(next_state, reward, done, _)\n",
    "            #print(done)\n",
    "            agent.step(action, reward, next_state, done)\n",
    "            state = next_state\n",
    "                \n",
    "            # within the episode loop\n",
    "            if (i_episode % display_freq == 1):\n",
    "                to_write = [task.sim.time]\n",
    "                to_write += list(task.sim.pose)\n",
    "                to_write += list(task.sim.v)\n",
    "                to_write += list(task.sim.angular_v)\n",
    "                to_write += list(action)   #rotor_speeds\n",
    "                to_write += [reward]                \n",
    "                for ii in range(len(labels)):\n",
    "                    results[labels[ii]].append(to_write[ii])\n",
    "                writer.writerow(to_write)\n",
    "        \n",
    "            if done:\n",
    "                # at the end of the episode, add the latest data points to the graph\n",
    "                #print(\"\\rEpisode = {:4d}, score = {:7.3f} (best = {:7.3f}), noise_scale = {}\".format(\n",
    "                #    i_episode, agent.score, agent.best_score, agent.noise_scale), end=\"\")  # [debug]\n",
    "                print(\"\\rEpisode = {:4d}\".format(i_episode), end=\"\")  # [debug]                                  \n",
    "                if (i_episode % display_freq == 1):    \n",
    "                    util.plot_run(results)\n",
    "                    print('__________________________________________________________________________________________')\n",
    "                total_rewards.append(reward.sum())   #append(reward.sum())\n",
    "                break\n",
    "        sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Plot the Rewards\n",
    "\n",
    "Once you are satisfied with your performance, plot the episode rewards, either from a single run, or averaged over multiple runs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "x and y must have same first dimension, but have shapes (1000,) and (437,)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-6d76b9d52444>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#plot episode rewards\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m13\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m7\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_episodes\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_rewards\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'episode total rewards'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\matplotlib\\pyplot.py\u001b[0m in \u001b[0;36mplot\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   3356\u001b[0m                       mplDeprecation)\n\u001b[0;32m   3357\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3358\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3359\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3360\u001b[0m         \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_hold\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwashold\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\matplotlib\\__init__.py\u001b[0m in \u001b[0;36minner\u001b[1;34m(ax, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1853\u001b[0m                         \u001b[1;34m\"the Matplotlib list!)\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlabel_namer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1854\u001b[0m                         RuntimeWarning, stacklevel=2)\n\u001b[1;32m-> 1855\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0max\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1856\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1857\u001b[0m         inner.__doc__ = _add_data_doc(inner.__doc__,\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\matplotlib\\axes\\_axes.py\u001b[0m in \u001b[0;36mplot\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1525\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcbook\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnormalize_kwargs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_alias_map\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1526\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1527\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_lines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1528\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_line\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1529\u001b[0m             \u001b[0mlines\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\matplotlib\\axes\\_base.py\u001b[0m in \u001b[0;36m_grab_next_args\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    404\u001b[0m                 \u001b[0mthis\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    405\u001b[0m                 \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 406\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mseg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_plot_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mthis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    407\u001b[0m                 \u001b[1;32myield\u001b[0m \u001b[0mseg\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    408\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\matplotlib\\axes\\_base.py\u001b[0m in \u001b[0;36m_plot_args\u001b[1;34m(self, tup, kwargs)\u001b[0m\n\u001b[0;32m    381\u001b[0m             \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindex_of\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    382\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 383\u001b[1;33m         \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_xy_from_xy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    384\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    385\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcommand\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'plot'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\matplotlib\\axes\\_base.py\u001b[0m in \u001b[0;36m_xy_from_xy\u001b[1;34m(self, x, y)\u001b[0m\n\u001b[0;32m    240\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    241\u001b[0m             raise ValueError(\"x and y must have same first dimension, but \"\n\u001b[1;32m--> 242\u001b[1;33m                              \"have shapes {} and {}\".format(x.shape, y.shape))\n\u001b[0m\u001b[0;32m    243\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    244\u001b[0m             raise ValueError(\"x and y can be no greater than 2-D, but have \"\n",
      "\u001b[1;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes (1000,) and (437,)"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 936x504 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plot episode rewards\n",
    "plt.figure(figsize=(13,7))\n",
    "plt.plot(range(1, num_episodes + 1), total_rewards, label='episode total rewards')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "util.plot_run(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Reflections\n",
    "\n",
    "**Question 1**: Describe the task that you specified in `task.py`.  How did you design the reward function?\n",
    "\n",
    "**Answer**: \n",
    "In `task.py` the task class is loaded. It initializes the PhysicsSim model, action repeats, and sets the interface to the cuadcopter parameters. It also sets up the reward function. \n",
    "\n",
    "I took the reward function \n",
    "* reward = 1. - .3 * (abs(self.sim.pose[:3] - self.target_pos)).sum()*\n",
    "\n",
    "It calculates the difference between target- and actual position. The bigger the difference, the lower the reward. I thought about taking the squared difference, but as it does not seem to be good in general to have big values for rewards, I did not try that in the end."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2**: Discuss your agent briefly, using the following questions as a guide:\n",
    "\n",
    "- What learning algorithm(s) did you try? What worked best for you?\n",
    "- What was your final choice of hyperparameters (such as $\\alpha$, $\\gamma$, $\\epsilon$, etc.)?\n",
    "- What neural network architecture did you use (if any)? Specify layers, sizes, activation functions, etc.\n",
    "\n",
    "**Answer**: I took the DDPG algorithm (**Continuous Control with Deep Reinforcement Learning**). This is a model-free algorithm based on deterministic policy gradient which is especially suited to operate over continuous action spaces. (DDPG = Deep Deterministic Policy Gradient)\n",
    "\n",
    "* Hyperparameters:\n",
    "I tried to choose most of the parameters as indicated in the *Experiment Details* section in the DDPG paper. The discount rate for the rewards was set to $\\gamma = 0.99$ and the learning rate for the soft target updates is set to $\\tau = 0,001$ The actor-NN final output layer activation function was set to $\\tanh$. \n",
    "\n",
    "I used 2 hidden layers of the "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3**: Using the episode rewards plot, discuss how the agent learned over time.\n",
    "\n",
    "- Was it an easy task to learn or hard?\n",
    "- Was there a gradual learning curve, or an aha moment?\n",
    "- How good was the final performance of the agent? (e.g. mean rewards over the last 10 episodes)\n",
    "\n",
    "**Answer**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4**: Briefly summarize your experience working on this project. You can use the following prompts for ideas.\n",
    "\n",
    "- What was the hardest part of the project? (e.g. getting started, plotting, specifying the task, etc.)\n",
    "- Did you find anything interesting in how the quadcopter or your agent behaved?\n",
    "\n",
    "**Answer**:"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
